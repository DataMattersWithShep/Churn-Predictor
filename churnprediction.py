# -*- coding: utf-8 -*-
"""ChurnPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gY5g5l586Pu_8wa6txeTaAJ3aGL2GpdG
"""

import pandas as pd
from google.colab import drive
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.metrics import accuracy_score

drive.mount('/content/drive')

data_descriptions = pd.read_csv('/content/drive/MyDrive/ChurnProject/data_descriptions.csv')
pd.set_option('display.max_colwidth', None)
data_descriptions

#Cols that are strings
# PaymentMethod, PaperlessBilling, MultiDeviceAccess, DeviceRegistered, GenrePreference, Gender, ParentalControl, SubtitlesEnabled, CustomerID
#Binary values
# PaperlessBilling, MultiDeviceAccess, Gender (male_yes), ParentalControl, SubtitlesEnabled

df = pd.read_csv("/content/drive/MyDrive/ChurnProject/train.csv")
df.head()

#Plot churn ratios
sns.histplot(data=df, x="Churn")

notchurned_counts, churned_counts = df.Churn.value_counts()
df_notchurned = df[df['Churn'] == 0]
df_churned = df[df['Churn'] == 1]
plt.style.use('seaborn')
print(df_notchurned)

temp_file = df_churned.sample(notchurned_counts, replace=True)
oversample_df = pd.concat([df_notchurned, temp_file], axis=0)
oversample_df.Churn.value_counts().plot(kind='bar', title='Sum of Rows When Oversampling', color=['red', 'blue'])

# Binarize columns and drop CustomerID
# Binary cols:
# PaperlessBilling, MultiDeviceAccess, Gender (male_yes), ParentalControl, SubtitlesEnabled
oversample_df['PaperlessBilling'] = (df['PaperlessBilling'] == "Yes").astype(int)
oversample_df['MultiDeviceAccess'] = (df['MultiDeviceAccess'] == "Yes").astype(int)
oversample_df['Male'] = (df['Gender'] == "Male").astype(int)
oversample_df = oversample_df.drop('Gender', axis = 1)
oversample_df['ParentalControl'] = (df['ParentalControl'] == "Yes").astype(int)
oversample_df['SubtitlesEnabled'] = (df['SubtitlesEnabled'] == "Yes").astype(int)
oversample_df = oversample_df.drop('CustomerID', axis = 1)
print(oversample_df)

#Cols that are non binary strings
# PaymentMethod, DeviceRegistered, GenrePreference, SubscriptionType


#First let's see how many unique values are in each column
unique_vals = oversample_df['PaymentMethod'].unique()
print(sorted(unique_vals))
#4

unique_vals = oversample_df['DeviceRegistered'].unique()
print(sorted(unique_vals))
#4

unique_vals = oversample_df['GenrePreference'].unique()
print(sorted(unique_vals))
#5

unique_vals = oversample_df['SubscriptionType'].unique()
print(sorted(unique_vals))
#3

# ContentType
unique_vals = oversample_df['ContentType'].unique()
print(sorted(unique_vals))
#3

# Then let's see what the distribution of each columns values looks like
# , color=['red', 'blue']
oversample_df.PaymentMethod.value_counts().plot(kind='bar', title='Dist of Payment Methhods')

oversample_df.DeviceRegistered.value_counts().plot(kind='bar', title='Dist of Registered Devices')

oversample_df.SubscriptionType.value_counts().plot(kind='bar', title='Dist of Subscription Types')

oversample_df.GenrePreference.value_counts().plot(kind='bar', title='Dist of Genre Preference')

#Pretty even distribution of all three column values, so let's vectorize these instead then produce a histagram
# PaymentMethod, DeviceRegistered, GenrePreference
oversample_df = pd.get_dummies(oversample_df, columns = ['SubscriptionType', 'ContentType', 'PaymentMethod', 'DeviceRegistered', 'GenrePreference'])
print(oversample_df)
# oversample_df = oversample_df.drop('B',axis = 1)

#Normalize dataset

cols = oversample_df.columns
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(oversample_df)
newdf_norm = pd.DataFrame(x_scaled, columns = cols)
print(newdf_norm)

plt.figure(figsize=(16,16))
sns.heatmap(newdf_norm.corr())

corr_df = newdf_norm.corr()
print(abs(corr_df["Churn"].sort_values()))
#Top 5 values that correlate with age
#AccountAge, AverageViewingDuration, ContentDownloadsPerMonth, ViewingHoursPerWeek, TotalCharges

# x = newdf_norm[['AccountAge', 'AverageViewingDuration', 'ContentDownloadsPerMonth', 'ViewingHoursPerWeek', 'TotalCharges']]
print(newdf_norm)
x = newdf_norm.drop(['Churn'], axis=1)
y = newdf_norm['Churn']

from sklearn.decomposition import PCA
import numpy as np

pca = PCA(n_components=21)

principalComponents = pca.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents)

print(np.array([pca.explained_variance_ratio_[:i].sum() for i in range(1, 21+1)]).round(2))

X_train, X_test, y_train, y_test = train_test_split(principalDf, y, test_size=0.3, random_state=42, stratify=y)
rf = RandomForestClassifier()
rf.fit(X_train, y_train)

predicted_results = rf.predict(X_test)
accuracy_score(y_test, predicted_results)

print(y_test)

print(predicted_results)

# from sklearn.metrics import roc_auc_score
# y_pred = (model.predict(X_test) > 0.5).astype("int32")
# print("Accuracy is " , accuracy_score(y_test, y_pred))
# print("ROC is " , roc_auc_score(y_test, y_pred))

# y_pred2 = (model2.predict(X_test2) > 0.5).astype("int32")
# accuracy_score(y_test2, y_pred2)